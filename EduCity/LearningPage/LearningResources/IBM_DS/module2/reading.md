
[font_size={50}][b]Module 2: Data Science on the Cloud[/b][/font_size]

[font_size={40}][b]Introduction[/b][/font_size]

Acquire technical expertise using popular open source data science frameworks including Jupyter notebooks and Python. 

[b]This page covers the following lecture details:[/b]
[ul]Duration[/ul]
[ul]Objectives[/ul]
[ul]Instructor[/ul]
[ul]Topics[/ul]
[ul]Skills[/ul]

[b]Upon completing this lecture you should be able to achieve the following objectives:[/b]
[ul]Understand the need for an integrated environment for Data Science Projects[/ul]
[ul]Explore the benefits of Data Science integrated environments hosted on the cloud [ul]
[ul]Explore how different data science roles participate in the data science lifecycle[ul]
[ul]Understand concepts such as: object storage, data refinery machine learning, visual recognition and model building[ul]
[ul]Evaluate the relationship between popular data science open source frameworks[ul]

[b]Skills:[/b]
[ul]Integrated Environment[/ul]
[ul]ML Framework[/ul]
[ul]Development Environment[/ul]
[ul]Data Sources[/ul]
[ul]Programming Languages[/ul]
[ul]Data Munging[/ul]
[ul]Visualization Tools[/ul]
[ul]Data Science lifecycle[/ul]
[ul]IBM Watson Studio[/ul]
[ul]Data Refinery[/ul]
[ul]AutoAI[/ul]



[font_size={40}][b][center]Topic 1: The need for cloud-based data science[/center][/b][/font_size]

In this topic, we'll talk about a few of the drivers, as well as the benefits, of adopting a cloud-based data science strategy. 


[font_size={30}][b]In what ways would cloud services benefit a data science team? [/b][/font_size]

    Cloud computing is bringing many data science benefits within reach of even small and midsized organizations.

    Data science’s foundation is the manipulation and analysis of extremely large data sets; the cloud provides access to storage infrastructures capable of handling large amounts of data with ease. Data science also involves running machine learning algorithms that demand massive processing power; the cloud makes available the high-performance computing that’s necessary for the task. To purchase equivalent on-site hardware would be far too expensive for many enterprises and research teams, but the cloud makes access affordable with per-use or subscription-based pricing.

    Cloud infrastructures can be accessed from anywhere in the world, making it possible for multiple groups of data scientists to share access to the data sets they’re working within the cloud—even if they’re located in different countries.

    Open source technologies are widely used in data science tool sets. When they’re hosted in the cloud, teams don’t need to install, configure, maintain, or update them locally. Several cloud providers also offer prepackaged tool kits that enable data scientists to build models without coding, further democratizing access to the innovations and insights that this discipline is making available.

    [b]Why are enterprises struggling to capture the value of data? [/b]

    They are collecting or sitting on vast amounts of data about their customers, suppliers, equipment, etc. They should be able to use this data to make better decisions, differentiated customer experiences, and better products and services. They also realize that they need to provide their frontline data users access to the data to get the speed that is needed in decision making, interactions, product recommendations, validation, etc. to be competitive.

    [b]The challenge that data science companies are facing is that:[/b]
    [ul]Data is in silos, hard to find, and difficult to use[/ul]
    [ul]Much of the data is not accessible to the frontline data users because the CDO/Compliance folks have it locked down to ensure it is used properly[/ul]
    [ul]Skills are a challenge[/ul]
    [ul]Infrastructure/Environments to get up and running aren’t there[/ul]

    [b]What is the solution?[/b]
    An integrated development environment for Data Science Projects:
    [ul]Data[/ul]
    [ul]Governance[/ul]
    [ul]Skills[/ul]
    [ul]Tools & Infrastructure[/ul]

    An integrated development environment provides a suite of tools for data scientists, application developers, and subject matter experts, allowing them to collaboratively connect to data, wrangle that data and use it to build, train and deploy models at scale. Successful AI projects require a combination of algorithms + data + teamwork, and a very powerful compute infrastructure.  

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/130917420[/url]


[font_size={30}][b]What are the challenges of closing the gap for data science adoption? [/b][/font_size]

    Data Science Cloud Platforms close the gap with a unified experience to create new insights from knowledge contained in the data, enabling multidisciplinary teams across the organization to collaborate.

    The problem is a gap between data experts and domain experts.

    [ul]Only highly technical professionals in IT could organize and make sense of the vast amounts of data.[/ul]
    [ul]Only domain experts could successfully convert data into the rich knowledge needed by AI.[/ul]
    [ul]But domain experts and IT professionals worked in silos, with different tools and no visibility to each others’ work.[/ul]
    [ul]The result was AI that fell short in its promise to augment people’s expertise.[/ul]

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/130917398[/url]


[font_size={30}][b]Summary[/b][/font_size]

    1)The challenge that data science companies are facing is that:
    [ul]Data is in silos, hard to find, and difficult to use[/ul]
    [ul]Much of the data is not accessible to the frontline data users because the CDO/Compliance folks have it locked down to ensure it is used properly[/ul]
    [ul]Skills are a challenge[/ul]
    [ul]Infrastructure/Environments to get up and running aren’t there[/ul]

    2)High specialized jobs combined with working in isolation often resulted in inefficient business practices.



[font_size={40}][b][center]Topic 2: End-to-end data science with IBM Watson Studio[/center][/b][/font_size]

In this topic, we'll explore the reasons for using data cleaning tools such as IBM Watson Data Refinery. 


[font_size={30}][b]Why might a data scientist use a data cleansing tool?[/b][/font_size]

    In order to apply AI, the first step of the workflow starts with connecting and accessing data. 

    Data scientists spend up to 80% of their time finding and preparing data, and 57% of data scientists said that cleaning and organizing data is the least enjoyable part of their job. The problem isn’t just limited to data scientists. Business analysts face similar struggles obtaining the data they need to build reports — often having to wait weeks for their IT team to extract data from the source systems.

    [b]Watson Studio – tools for supporting the end-to-end data science workflow[/b]

    To address the issue, we provide the integrated capability to refine and wrangle data with Data Refinery, a tool that makes fast, self-service data preparation a reality. Watson Studio comes with more than 35 data connectors to the most popular data sources, whether they are in the IBM Cloud, 3rd party Clouds or application, or on-premises. 

    [b]Watson Studio – tools for supporting the end-to-end data science workflow[/b]

    Organizations are now tapping data science and artificial intelligence (AI) as a technology-enabled business strategy. Experimentation is accelerating across multiple clouds. The need for speeding through data preparation and exploration, modeling, and training has never been higher. 

    To succeed in enterprise AI, a business must:
    [ul]Bring your algorithms to wherever data resides.[/ul]
    [ul]Increase productivity of data scientists, analysts, developers, and subject matter experts.[/ul]
    [ul]Operationalize the data science lifecycle from insight to prediction and optimization.[/ul]

    Together with IBM Watson Machine Learning, IBM Watson Studio is a leading data science and machine learning platform built from the ground up for an AI-powered business. It helps enterprises simplify the process of experimentation to deployment, speed data exploration, and model development and training, and scale data science operations across the lifecycle. 

    IBM Watson Studio empowers organizations to tap into data assets and inject predictions into business processes and modern applications and then optimize business value with visual data science and decision optimization. It's suited for hybrid multi-cloud environments that demand mission-critical performance, security, and governance — in public clouds, in private clouds, on-premises, and on the desktop, including IBM Cloud Pak™ for Data. 

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/128535658[/url]


[font_size={30}][b]What are the capabilities of data cleansing tools such as IBM Watson Data Refinery?[/b][/font_size]

    Watson Studio – differentiating capabilities

    [b]Bringing it all together[/b]
    Watch the video below to better understand this product offering:
    [url=]https://video.ibm.com/embed/recorded/128536448[/url]


[font_size={30}][b]Summary[/b][/font_size]

    1)Data cleaning is often the most time consuming and least enjoyable part of a data scientist's work. 

    2)Data cleansing tools use AI algorithms to make the process of data cleansing faster and more efficient. 



[font_size={40}][b][center]Topic 3: Data Science lifecycle: Exploration Phase[/center][/b][/font_size]

In this topic, we'll introduce you to the data science lifecycle and walk you through the three stages of the data exploration phase: exploration, preparation, model development.


[font_size={30}][b]Describe the three main steps of the exploration phase within the data science lifecycle.[/b][/font_size]

    It's difficult for today's data science teams to respond fast enough to the demands of their business. With the explosion of data from multiple sources and formats, trying to develop models and putting them into production quickly is a huge challenge for many teams. 

    This guide provides an overview of the typical data science lifecycle, common challenges organizations encounter, and how IBM® Watson Studio can address them-enabling your data science team to accelerate and optimize the value of analytics results throughout your organization.

    [b]Bringing the Data Science methodology, roles, and tools into a Data Science lifecycle[/b]

    The data science lifecycle—also called the data science pipeline—includes anywhere from five to sixteen (depending on whom you ask) overlapping, continuing processes. 

    [b]The processes common to just about everyone’s definition of the lifecycle include the following:[/b]

        [b]1)Capture:[/b] This is the gathering of raw structured and unstructured data from all relevant sources via just about any method—from manual entry and web scraping to capturing data from systems and devices in real time.

        [b]2)Prepare and maintain:[/b] This involves putting the raw data into a consistent format for analytics or machine learning or deep learning models. This can include everything from cleansing, deduplicating, and reformatting the data, to using ETL (extract, transform, load) or other data integration technologies to combine the data into a data warehouse, data lake, or other unified store for analysis.

        [b]3)Preprocess or process:[/b] Here, data scientists examine biases, patterns, ranges, and distributions of values within the data to determine the data’s suitability for use with predictive analytics, machine learning, and/or deep learning algorithms (or other analytical methods).

        [b]4)Analyze:[/b] This is where the discovery happens—where data scientists perform statistical analysis, predictive analytics, regression, machine learning and deep learning algorithms, and more to extract insights from the prepared data.

        [b]5)Communicate:[/b] Finally, the insights are presented as reports, charts, and other data visualizations that make the insights—and their impact on the business—easier for decision-makers to understand. A data science programming language such as R or Python (see below) includes components for generating visualizations; alternatively, data scientists can use dedicated visualization tools.

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/128535653[/url]

    The exploration phase of the data science lifecycle is comprised of three stages: Data Preparation, Data Exploration, and Model Development.

    [b]Exploration phase : Big Data and Analytics[/b]

    [b]Data Exploration[/b]

    Once your data is in the right format to work with, you can conduct the next phase in the data analysis process. This initial exploration of the dataset is critical.

    After you complete the up-front tasks of translating a business problem into an AI and data science solution, and understanding the data needs in support of your business problem, it’s time to prepare the data. You need to prepare the data in a format that can be used for model development, measurement, and training a machine learning model.

    Most AI and data science models require data to be combined and denormalized into one large analytical record before data mining, feature selection, model development and optimization, and training can occur.

    [b]Data Preparation[/b]

    Uncover hidden insights in your data with the Data Refinery tool, which provides built-in data cleaning and transformations. 

    Gain access to a tabular view of your data, including visualizations and summary statistics that can help you uncover hidden insights by asking the right business questions of your data.

    [b]Data preparation involves these tasks:[/b]
    [ul]Select a sample subset of data[/ul]
    [ul]Filter on rows that target particular customers or products that help answer the data analysis and business goals. Also, filter on attributes that are relevant to data analysis and business goals. Some data science patterns, such as Fraud Detection, AML Anti-Money Laundering, and Log Analysis might require full-volume, unsampled data.[/ul]
    [ul]Merge data sets or records[/ul]
    [ul]To join data sets, you need a common key. Aggregate records, and merge based on group by like operations.[/ul]
    [ul]Derive new attributes[/ul]
    [ul]When you merge data sets, especially when you have many relationships, it can be useful to derive new attributes. For example, if you have a customer data set and a customer purchases data set, you might condense the purchases to a new derived attribute with mean or total spending.[/ul]
    [ul]Format and sort the data for modeling[/ul]
    [ul]Sequence and temporal algorithms might need data to be presorted into a particular order. Categorical data fields might need to be converted from textual categories to numerical ones.[/ul]
    [ul]Remove or replace blank or missing values[/ul]
    [ul]Exclude rows where the missing attribute is key in making the decision. Fill in missing attributes with 0 or estimated values where the rest of the row adds value to the analysis.[/ul]
    [ul]If your data is sparse or you have many attributes, consider reducing the number of features. Principle Component Analysis can show you which attributes have the biggest impact on the data. Linear regression can show you that two attributes are correlated, so you need to use only one of those attributes and remove the other.[/ul]
    [ul]Normalize numeric fields to use the same range. Features with large values compared to features with small values can be given more weight by various algorithms. Normalization eliminates the unit of measurement by rescaling data, often to a value in the range 0 - 1.[/ul]
    [ul]Replace or correct data and measurement errors[/ul]

    Don't worry if these terms seem a bit heavy, we will dive deeper into these in the intermediate and advanced courses. 

    [b]Data Refinery[/b]

    Data Refinery is an essential and early-on task that a data wrangler will undertake to cleanse and transform their data.

    The Data Refinery tool, available via Watson Studio and Watson Knowledge Catalog, saves data preparation time by quickly transforming large amounts of raw data into consumable, quality information that’s ready for analytics.

    The Data Refinery service reduces the amount of time it takes to prepare data. Use pre-defined operations that you can use in your data flows to transform large amounts of raw data into consumable, quality data that’s ready for analysis. 

    [b]With Data preparation tools like IBM's Data Refinery, you can:[/b]
    [ul]Interactively discover, cleanse, and transform your data with over 100 built-in operations. No coding is required.[/ul]
    [ul]Understand the quality and distribution of your data using dozens of built-in charts, graphs, and statistics.[/ul]
    [ul]Automatically detect data types and business classifications.[/ul]
    [ul]Schedule data flow executions for repeatable outcomes.[/ul]

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/130917484[/url]


[font_size={30}][b]What are the tasks involved in data exploration?[/b][/font_size]

    Once your data is in the right format to work with, you can conduct the next step in the data analysis process: data exploration.

    This initial exploration of the dataset is critical because it helps data scientists illuminate previously unknown patterns, relationships, or other actionable findings. 

    [b]Data Exploration[/b]

    Using the included dashboarding service, produce stunning visualizations directly from your data in real-time. 

    This allows you to illuminate previously unknown patterns, relationships, or other actionable findings, and easily share them with your team.

    Exploratory data analysis (EDA) is a technique used for data exploration to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.

    EDA is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task and provides a better understanding of data set variables and the relationships between them. It can also help determine if the statistical techniques you are considering for data analysis are appropriate.

    [b]Some helpful questions to ask at this point include:[/b]
    [ul]Which attributes seem promising for further analysis?[/ul]
    [ul]Has the exploration revealed new characteristics about the data?[/ul]
    [ul]How have these explorations changed any initial hypotheses?[/ul]
    [ul]Can a specific subset of the data be used later?[/ul]
    [ul]Has the data exploration altered the project goals?[/ul]

    The main purpose of EDA is to help look at data before making any assumptions. It can help identify obvious errors, as well as better understand patterns within the data, detect outliers or anomalous events, find interesting relations among the variables.

    Data scientists can use exploratory analysis to ensure the results they produce are valid and applicable to any desired business outcomes and goals. EDA also helps stakeholders by confirming they are asking the right questions. EDA can help answer questions about standard deviations, categorical variables, and confidence intervals. Once EDA is complete and insights are drawn, its features can then be used for more sophisticated data analysis or modeling, including machine learning (Artificial Intelligence) programs.

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/130917494[/url]


[font_size={30}][b]Summary[/b][/font_size]

    1)Data preparation (including data cleansing and data wrangling) is one of the most important and time-consuming aspects of data science. It is essential to first filter the data that you're working with to "weed out" the noise and organize data for easier processing. 

    2)Once your data is in the right format to work with, you can conduct the next step in the
    data analysis process: data exploration.



[font_size={40}][b][center]Topic 4: The role of the data analyst in an integrated Cloud environment[/center][/b][/font_size]

In this topic, we'll explore the role of the data analyst within the data science lifecycle. 


[font_size={30}][b]What role does the data analyst play in the data exploration phase? [/b][/font_size]

    In practice, several people work on a team to build data products. 

    Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data. 

    [b]Data Analyst role in the integrated environment[/b]

        [b]Sally: a Data Analyst[/b]

        [b]Her Job:[/b]
        Captures domain knowledge for successful business alignment.

        [b]What she does:[/b]
        [ul]She works with the domain experts to understand the business problems.[/ul]
        [ul]Provides domain knowledge that Maria and Tom use to develop custom data models.[/ul]
        [ul]Sally uses data refinery tools to cleanse and curate the raw data.[/ul]
        [ul]Makes data visualizations to depict insights to sponsoring users.[/ul]


    Because data scientists are involved in each step of the journey in building data products, they tend to bring a holistic view to solving problems with data. However, they can’t be experts in everything—this is where their team can help. Skilled data analysts, scientists, data engineers, developers, and business analysts are transformative figures in modern business. They are the beating heart of the big data economy. It’s not just that they are designing new systems; they are going to bat for new sources of data and new ways to use that data. Of course, IT still must build the system, but the data science professionals are the ones who help departments collaborate to solve problems and speed innovation. 

    [b]Predicting fraud with Data[/b]

        [b]Sally: a Data Analyst[/b]

        [b]Problem[/b]
        Organizations are trying to predict fraud or suspicious activity and their patterns to help drastically reduce losses.

        [b]Challenge[/b]
        [ul]How to make sense of the structured data?[/ul]
        [ul]Where is the signal and where is the noise?[/ul]

        [b]Solution[/b]
        Data Refinery tools !

    [b]The Data Analyst: Consider the following use case. [/b]

    During its fifty-year history, an insurance company has been struggling to detect potentially fraudulent activity and has turned to data science to predict fraud within insurance claims before the claim is settled.

    Before long, a data analyst is given a giant spreadsheet of a thousand rows and hundreds of columns. The Analyst takes the spreadsheet and begins the task of data wrangling, cleansing, and curating the records within. Are there any missing values? Should they be removed, or the missing value must be provided? Are any of the columns redundant can they be combined? Or removed altogether?

    The Profile tab of the Data Refinery tool can help in highlighting those redundant columns, or noise if you will, from the data set. These characteristics of the data set are best revealed with summary statistics such as min, max, median and standard deviation. Summary statistics make the job of the data analyst much easier and accurate by visually revealing the noise from the signal. 

    [b]Statistical Analysis[/b]

        [b]Explanation:[/b] Sally first compared the amount of days since the loss happened (traffic accident) with the amount of days left for policy or license expiration, then the column SUSPICIOUS_CLAIMS_FLAG was built to support analysis of Fraudulent Claims using logistic regression as a binary (two-class) classification problem.

        [b]Analysis outcome:[/b] Used for future prediction
        Sally through graphs, and statistics, found the following patterns:
        [ul]Fraud is more likely to occur with folks that submit multiple claims in a year.[/ul]
        [ul]The greater the claim amount, the more likely of fraudulent behavior.[/ul]

    [b]Dynamic Dashboards – making insights available to all[/b]

    Our brains are naturally more adept at processing visual data as opposed to strings of numbers. Therefore, data scientists commonly use data visualizations (graphs, charts, etc.) to quickly view relevant features of their datasets and identify variables that are likely to result in interesting observations. 

    By displaying data graphically-for example, through scatter plots or bar charts-users can see if two or more variables correlate and determine if they are good candidates for more in-depth analysis. 

    [b]There are four primary types of Exploratory Data Analysis:[/b]

        [b]1)Univariate non-graphical:[/b]
        This is the simplest form of data analysis, where the data being analyzed consists of just one variable. Since it’s a single variable, it doesn’t deal with causes or relationships. The main purpose of the univariate analysis is to describe the data and find patterns that exist within it. 

        [b]2)Univariate graphical:[/b]
        Non-graphical methods don’t provide a full picture of the data. Graphical methods are therefore required. Common types of univariate graphics include:                      
        [ul][b]Stem-and-leaf plots,[/b] which show all data values and the shape of the distribution.[/ul]
        [ul][b]Histograms,[/b] a bar plot in which each bar represents the frequency (count) or proportion (count/total count) of cases for a range of values.[/ul]
        [ul][b]Box plots,[/b] which graphically depict the five-number summary of minimum, first quartile, median, third quartile, and maximum.[/ul]

        [b]3)Multivariate nongraphical:[/b]
        Multivariate data arises from more than one variable. Multivariate non-graphical EDA techniques generally show the relationship between two or more variables of the data through cross-tabulation or statistics. 

        [b]4)Multivariate graphical:[/b]
        Multivariate data uses graphics to display relationships between two or more sets of data. The most used graphic is a grouped bar plot or bar chart with each group representing one level of one of the variables and each bar within a group representing the levels of the other variable.
        Other common types of multivariate graphics include:
        [ul]Scatter plot, which is used to plot data points on a horizontal and a vertical axis to show how much one variable is affected by another.[/ul]
        [ul]Multivariate chart, which is a graphical representation of the relationships between factors and response.[/ul]
        [ul]Run chart, which is a line graph of data plotted over time.[/ul]
        [ul]Bubble chart, which is a data visualization that displays multiple circles (bubbles) in a two-dimensional plot.[/ul]
        [ul]Heat map, which is a graphical representation of data where values are depicted by color.[/ul]


[font_size={30}][b]Summary[/b][/font_size]

    1)The data analyst brings the claims data into Watson Studio where she begins work on analyzing, cleaning, and wrangling the data into something that can be understood by a machine learning model. 



[font_size={40}][b][center]Topic 5: The role of the data scientist in an integrated Cloud environment[/center][/b][/font_size]

In this topic, we'll explore the role of the data scientist in developing an AI model using Watson Studio. 


[font_size={30}][b]What is a data science model? [/b][/font_size]

    In practice, several people work on a team to build data products. Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data.

    Because data scientists are involved in each step of the journey in building data products, they tend to bring a holistic view to solving problems with data. However, they can’t be experts in everything—this is where their team can help. 

    [b]Data scientist role in the integrated environment[/b]

        [b]Data scientist role in the integrated environment[/b]

        [b]Maria: a Data Scientist[/b]

        [b]Her Job:[/b]
        Transform data into knowledge to solve business problems.

        [b]What she does:[/b]
        [ul]Follow the project end-to-end.[/ul]
        [ul]Discover content from multiple data sources.[/ul]
        [ul]Use popular statistical libraries.[/ul]
        [ul]Run experiments to build custom models that solve business problems.[/ul]
        [ul]Use techniques such as Machine Learning or Deep Learning and works with Sally to validate the  success of trained models.[/ul]

    Skilled data scientists, data engineers, developers, and business analysts are transformative figures in modern business. They are the beating heart of the big data economy. It’s not just that they are designing new systems; they are going to bat for new sources of data and new ways to use that data. Of course, IT still must build the system, but the data science professionals are the ones who help departments collaborate to solve problems and speed innovation. 

    You defined business goals and spent hours digging through data. You even did a bit of "data wrangling" to handle missing values and convert the job experience categorical data field into something numerical. 

    Now, what are you going to do with all that data, and how can what you do with it add value to the business? It's time to create a model.

    [b]Predictive analytics through data modeling techniques[/b]

        [b]Maria:[/b] a Data Scientist

        [b]Problem:[/b] Now that we have sanitized and curated the data.

        [b]Challenge:[/b] What predictions can we make? 

        [b]Solution:[/b] Data Models!

    Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. The goal is to illustrate the types of data used and stored within the system, the relationships among these data types, the ways the data can be grouped and organized and its formats and attributes.

    [b]Data models are built around business needs. [/b]
    Rules and requirements are defined upfront through feedback from business stakeholders so they can be incorporated into the design of a new system or adapted in the iteration of an existing one.

    [b]Data can be modeled at various levels of abstraction. [/b]
    The process begins by collecting information about business requirements from stakeholders and end users. These business rules are then translated into data structures to formulate a concrete database design. A data model can be compared to a roadmap, an architect’s blueprint or any formal diagram that facilitates a deeper understanding of what is being designed.

    [b]Data modeling employs standardized schemas and formal techniques.[/b]
    This provides a common, consistent, and predictable way of defining and managing data resources across an organization, or even beyond. Ideally, data models are living documents that evolve along with changing business needs. They play an important role in supporting business processes and planning IT architecture and strategy. Data models can be shared with vendors, partners, and/or industry peers.


[font_size={30}][b]What tasks must the data scientist perform in order to develop a machine learning model for predicting fraudulent claims? [/b][/font_size]

    This is the point at which your hard work begins to pay off.  

    The data you spent time preparing is brought into the data science toolset, and the results begin to shed some light on the business problem posed during the early stages of the project.  

    [b]Exploration phase: AI methodologies and Tools for Data Science[/b]

    [b]Model Development[/b]

    Here, the data you've prepared is brought into the data science toolset, and the results begin to shed some light on previously identified business problems.
    [ul]Test and deploy models using customizable compute environments that scale up and down with your workflow. [/ul]
    [ul]Choose from various capabilities of Anaconda, Apache Spark, and GPU environments.[/ul]

    Model development is usually conducted in multiple iterations. Typically, data scientists run several models using default parameters and then fine-tune the parameters or revert to the data preparation phase for manipulations required by their model of choice. It's rare for an organization's question to be answered satisfactorily with a single algorithm and a single execution. This is what makes data science so interesting. There are many ways to look at a given problem, and today there are a wide variety of tools to help you do that.

    Although you may already have some idea about which types of models are most appropriate for your organization's needs, now is the time to make some decisions about which ones to use. Determining the most appropriate model will typically be based on the data types available, your project goals, and specific requirements for data sizes or types. 

    [b]Types of Data Models[/b]

    Data modeling has evolved alongside database management systems, with model types increasing in complexity as businesses' data storage needs have grown. Here are several model types: 

        [b]1)Hierarchical data models:[/b]
        Hierarchical data models represent one-to-many relationships in a treelike format. In this type of model, each record has a single root or parent which maps to one or more child tables. This model was implemented in the IBM Information Management System (IMS), which was introduced in 1966 and rapidly found widespread use, especially in banking. Though this approach is less efficient than more recently developed database models, it’s still used in Extensible Markup Language (XML) systems and geographic information systems (GISs). 

        [b]2)Relational data models:[/b]
        Relational data models were initially proposed by IBM researcher E.F. Codd in 1970. They are still implemented today in the many different relational databases commonly used in enterprise computing. Relational data modeling doesn’t require a detailed understanding of the physical properties of the data storage being used. In it, data segments are explicitly joined through the use of tables, reducing database complexity. Relational databases frequently employ structured query language (SQL) for data management. These databases work well for maintaining data integrity and minimizing redundancy. They’re often used in point-of-sale systems, as well as for other types of transaction processing. 

        [b]3)Entity-relationship (ER) data models:[/b]
        Entity-relationship (ER) data models use formal diagrams to represent the relationships between entities in a database. Several ER modeling tools are used by data architects to create visual maps that convey database design objectives. 

        [b]4)Object-oriented data models:[/b]
        Object-oriented data models gained traction as object-oriented programming and it became popular in the mid-1990s. The “objects” involved are abstractions of real-world entities. Objects are grouped in class hierarchies, and have associated features. Object-oriented databases can incorporate tables, but can also support more complex data relationships. This approach is employed in multimedia and hypertext databases as well as other use cases.

        [b]5)Dimensional data models:[/b]
        Dimensional data models were developed by Ralph Kimball, and they were designed to optimize data retrieval speeds for analytic purposes in a data warehouse. While relational and ER models emphasize efficient storage, dimensional models increase redundancy in order to make it easier to locate information for reporting and retrieval. This modeling is typically used across OLAP systems.

    Two popular dimensional data models are the star schema, in which data is organized into facts (measurable items) and dimensions (reference information), where each fact is surrounded by its associated dimensions in a star-like pattern. The other is the snowflake schema, which resembles the star schema but includes additional layers of associated dimensions, making the branching pattern more complex. 

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/130917502[/url]

    [b]AutoAI applies various algorithms to prepare your raw data for machine learning[/b]

    The AutoAI process follows this sequence to build candidate pipelines:
    [ul]Data pre-processing[/ul]
    [ul]Automated model selection[/ul]
    [ul]Automated feature engineering[/ul]
    [ul]Hyperparameter optimization[/ul]

    [b]Data pre-processing[/b]
    Most data sets contain different data formats and missing values, but standard machine learning algorithms work with numbers and no missing values. AutoAI applies various algorithms to analyze, clean, and prepare your raw data for machine learning. It automatically detects and categorizes features based on data type, such as categorical or numerical. Depending on the categorization, it uses hyper-parameter optimization to determine the best combination of strategies for missing value imputation, feature encoding, and feature scaling for your data. 

    [b]Build custom ML models using Open-Source libraries[/b]
    Create ML flows and design Neural Networks visually

    [b]Data Analysis Outcomes[/b]

    In Watson Studio, you can design your neural networks such as the pooling, ReLU, convolution and full circle stages, and you can do so using numerous open-source frameworks such as TensorFlow, Keras, Caffe and PyTorch.


[font_size={30}][b]Summary[/b][/font_size]

    1)A data model identifies the data, the data attributes, and the relationships or associations with other data. It provides a generalized, user-defined view of data that represents the real business scenario and data. 

    2)Model development is usually conducted in multiple iterations. Typically, data scientists run several models using default parameters and then fine-tune the parameters or revert to the data preparation phase for manipulations required by their model of choice. 



[font_size={40}][b][center]Topic 6: The role of the data engineer in an integrated Cloud environment[/center][/b][/font_size]

In this topic, we'll explore the unique role of the data engineer within the data science lifecycle. 


[font_size={30}][b]What are the primary roles of the data engineer, and how do they differ from the data analyst and data scientist? [/b][/font_size]

    Big Data Engineers focus on collecting, parsing, managing and analyzing large data sets, in order to provide the right data sets and visual tools for analysis to the data scientists. 

    They understand the complexity of data and can handle different data varieties (structured, semi-structured, unstructured), volume, velocity (including stream processing), and veracity. They also address the information governance and security challenges associated with the data. They have a good background in software engineering and extensive programming and scripting experience. 

    [b]Roles integrated environment[/b]

        [b]Tom: a Data Engineer[/b]

        [b]His Job:[/b] 
        Architects how data is organized and ensures operability.

        [b]What he does:[/b]
        [ul]Uses tools for data munging and data wrangling to develop pipelines that Maria can use to apply statistical methods and build models.[/ul]
        [ul]Works with Maria to transform research models into production quality-systems.[/ul]
        [ul]Builds data infrastructure and ETL pipelines. Works with Spark, Hadoop, and HDFS.[/ul]

        [b]Responsibilities:[/b]
        Manage the Model lifecycle end-to-end.

    [b]Data preparation: Extract and transform data into usable formats[/b]

    Data engineers must understand how to finesse the flow of data to minimize movement latencies and bring agility to analytics. They also work with front-end developers when moving data science projects into production. In many organizations, a data engineer will be in charge of integrating data, including designing, building and measuring data ingestion and integration pipelines for large volumes of temporal data from different sources. Examples include database 3 The data engineer extracts, application server logs, scanned images, voice recordings, Twitter streams, websites and health sensor data. Once continuous pipelines are installed—to and from these huge “pools” of filtered information—data scientists can pull relevant data sets for their analyses.  

    [b]Data Science integrated environment layers[/b]

    Data engineers are often looking to deliver actionable insights through the marriage of structured and unstructured data. Unfortunately, you can’t assume that the data—even structured data—is perfect and ready to use. It may need some work:
    [ul]Data may be incomplete, with missing or incorrect values[/ul]
    [ul]Data can be corrupted or incoherent, with broken lines or fields in the wrong place[/ul]
    [ul]Data may need to be transformed or standardized into an algorithm- or model-friendly format[/ul]
    [ul]Data may have too much “noise”: random, irrelevant data that throws off analytics[/ul]

    First, you have to capture and preserve the data structure as you move it into your Hadoop environment. Next, you’ll need to bring along your metadata, particularly business metadata around critical terms, semantic meaning and business context. The goal is to relate different data sets to find insights. One approach is data tagging.

    Tagging data before handing it off to Hadoop, Spark or another implementation medium for analysis can save an enormous amount of time. The trick is to process the data “just enough” so that it is useful when loaded into a big data environment—able to be discovered, accessed and joined with Data preparation: Extract and transform data into usable formats other data. Later, if you’re looking to operationalize a useful analysis, it may be worthwhile to do more data preparation work at that time.

    [b]Watson Studio[/b]

    [b]Data science is a rapidly evolving discipline that leverages an ever-widening array of tools and capabilities to learn and exploit data. Because of such inherent complexities surrounding adoption, integration and support, the work of the data scientist can be daunting.[/b]

    That complexity is one of the reasons IBM several years ago set out to bring clarity and uniformity to the otherwise disparate data discovery and analytics process.

    [b]The goal:[/b] create a solution that leveraged the best capabilities available, in an integrated, collaborative platform that was easy to access and use. With it, everyone from data scientists to business analysts would be able to not only tackle the discipline but conquer it.

    Up until the time IBM rolled out the popular IBM Watson Studio, if someone wanted to engage in data science, he/she would have to search the web, review components like Jupyter notebooks, or development platforms like Scala and R, big data tools like Hadoop, and much more – and then learn how to use them. Not unexpectedly, the wide variety of tools and programs led to relatively slow adoption, challenging integration, and cumbersome support.

    In addition, IBM research showed that once up and running, most data scientists’ workflows were often fragmented, requiring them to toggle between a variety of workspaces and tools to complete a job. For example, they might use Data Shaper to clean data, Jupyter for modeling and MatPlotLib for visualization. These tools support a linear process, but data scientists’ workflows are more cyclical.

    When IBM launched the IBM Watson Studio in 2016, users for the first time had a solution that integrated the most sought-after development, notebook and analytics tools, in a simple-yet-scalable web-based platform. It also enabled users to connect live with IBM for all support.

    In addition to these goals, the vision for IBM Watson Studio was that it would accommodate agile workflow, simplify the experience of working with data, and bring all the tools into a unified data ecosystem. IBM Watson Studio, in function and form, helps simplify the data science universe. Today, Data Science Experience is one of the premier data science systems available on the market, with thousands of users worldwide.

    [ul]Notice that with Watson Studio as an integrated environment, you can run your choice of Python or R as your analysis package.[/ul]
    [ul]Additionally, you can customize your compute environment moving from CPU to GPU and modifying storage capacity.[/ul]
    [ul]You can seamlessly incorporate Visual Recognition service within Watson Studio to work with images.[/ul]
    [ul]That includes the ability to customize the machine learning flow regarding nodes and hyperparameters. [/ul]

    [b]Employ latest neural networks to predict and build patterns[/b]

    You can submit your deployment to the Watson Machine Learning service and let the system recommend the optimal algorithm such as eXtreme Gradient Boost or Random Forest Trees. 


[font_size={30}][b]Summary[/b][/font_size]

    1)Big Data Engineers focus on collecting, parsing, managing and analyzing large data sets, in order to provide the right data sets and visual tools for analysis to the data scientists. 



[font_size={40}][b][center]Topic 7: Tools to support the Data  Science lifecycle - Production Phase[/center][/b][/font_size]

In this topic, we'll explore the 3 stages that make up the production phase of the data science lifecycle.


[font_size={30}][b]Describe the three main steps of the production phase within the data science lifecycle. [/b][/font_size]

    The production phase includes the model implementation, model deployment, and model management stages. 

    This phase in the data science project is focused on model testing through environmental exposure. Here is where the data science team can validate the level of accuracy of the model developed and trigger a loop-back event for model optimization.

    [b]Production phase[/b]

    [b]Model implementation [/b]

    Improve your model's performance by visualizing the fit between the model and data using the model visualization capabilities of IBM SPSS® Modeler.

    Your models have been built, and you've selected the best one. Before actual deployment, however, you need to evaluate your model to understand its quality and ensure that it fully addresses the business problem. Model implementation entails computing various diagnostic measures and other outputs such as tables and graphs, enabling the data scientist to interpret the model's quality and its efficacy in solving the problem.

    For a predictive model, data scientists use a testing set that is independent of the training set but follows the same probability distribution and has a known outcome. The testing set is used to evaluate the model so it can be refined as needed. Sometimes the final model is applied also to a validation set for a final assessment.

    Data scientists may also assign statistical significance tests to the model as further proof of its quality. This additional proof may be instrumental in justifying model implementation or taking actions when the stakes are high-such as an expensive medical protocol or a critical airplane flight system.

    [b]Model Deployment[/b]

    Once your model is ready, deploy and score it with the available Watson Machine Learning service.

    Once a satisfactory model has been developed and is approved by the business sponsors, you are ready to deploy it into the production environment or a comparable test environment. 

    Deploying a model into an operational business process is usually an IT function, although it can involve additional groups. IT departments often have development, testing, and staging environments. However, the process is not as simple as handing the model off to your IT team. Testing the model first is necessary to identify any dependencies in the production environment. In addition, data science teams need to ensure models receive the correct production data and send the scores to the right place and that the system must be set up for monitoring and scalability.

    Questions to ask at this stage of the process include:
    [ul]What data does the model expect to ingest and produce?[/ul]
    [ul]What infrastructure is required to run this model at scale?[/ul]
    [ul]Does the model have everything it needs to execute?[/ul]
    [ul]How easy is it to identify points of failure if the model is not running properly?[/ul]
    [ul]What is the feedback process from the production users to the Data Science Team?[/ul]

    Models are normally deployed in a limited way until their performance has been fully evaluated. Deployment may be as simple as generating a report with recommendations or as involved as embedding the model in a complex workflow and scoring process managed by a custom application. 

    [b]Model management[/b]

    Compare runs and conduct model hyperparameter optimization easily with deep learning experiments.

    During deployment and integration of modeling results, model management must be a continuous process to ensure optimal performance over time. This means knowing exactly where each model is in the lifecycle, how old the model is, who developed it, and who is using it for which application. 

    Model version control, which includes event logging and change tracking to understand how the model evolves over time, is another critical requirement.Data science must also be concerned with model decay and continuously gather metrics to determine when a model should be refreshed or replaced. For example, a model deployed to increase customer retention among high-value customers will need to be revised once a level of retention is reached.

    [b]Bringing it all together[/b]
    In the video below your instructor will guide you through these concepts:
    [url=]https://video.ibm.com/embed/recorded/128535656[/url]


[font_size={30}][b]Summary[/b][/font_size]

    1)The production phase includes the Model implementation, model deployment, and model management stages. This phase in the data science project is focused on model testing through environmental exposure. 

    2)Model implementation entails computing various diagnostic measures and other outputs such as tables and graphs, enabling the data scientist to interpret the model's quality and its efficacy in solving the problem. 

    3)After development and implementation comes the process of running the model through a series of extensive tests and finally making it ready for production. 



[font_size={40}][b][center]Summary of Module 2[/center][/b][/font_size]

1)Data Science Cloud Platforms offer a unified experience, enabling multidisciplinary teams across the organization to collaborate.

2)The benefits of a data science integrated environment on the cloud are:
[ul]Allows different roles to use popular statistical libraries[/ul]
[ul]Seamlessly runs experiments to build custom models that solve business problems.[/ul]
[ul]Use techniques such as Machine Learning or Deep Learning to validate the success of the trained models.[/ul]
[ul]Easily export the information into data visualizations to depict insights to sponsor users[/ul]

3)Communities are can contribute with datasets, models and enhance collaboration in complex data science projects.
